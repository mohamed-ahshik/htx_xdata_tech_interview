{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c8d1f9c",
   "metadata": {},
   "source": [
    "# Link Analysis 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12accb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "import numpy as np\n",
    "import math\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "\n",
    "sys.path.insert(0,'/Users/user/Documents/htx_xdata/htx_xdata_tech_interview')\n",
    "\n",
    "from src.preprocess import preprocess_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a9d5f6",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93e491f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates :210\n",
      "After dropping Duplicates :0\n",
      "Column : resale_price datatype before converting : float64\n",
      "Column : resale_price datatype after converting : int64\n",
      "Column : floor_area_sqm datatype before converting : float64\n",
      "Column : floor_area_sqm datatype after converting : int64\n",
      "Column : remaining_lease datatype before converting : object\n",
      "Column : remaining_lease datatype after converting : int64\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Data path\n",
    "registration_date_jan_2017_path = r'/Users/user/Documents/htx_xdata/htx_xdata_tech_interview/data/HDB/resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv'\n",
    "\n",
    "# Load data into pandas dataframe\n",
    "registration_date_jan_2017_df = pd.read_csv(registration_date_jan_2017_path)\n",
    "\n",
    "# Preprocess the data\n",
    "registration_date_jan_2017_df_cleaned = preprocess_pipeline(registration_date_jan_2017_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2df5d8a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
       "       'floor_area_sqm', 'flat_model', 'lease_commence_date',\n",
       "       'remaining_lease', 'resale_price', 'month_only', 'year_only',\n",
       "       'floor_area_sqft', 'storey_min', 'storey_max', 'psf_temp', 'psf',\n",
       "       'address', 'road'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registration_date_jan_2017_df_cleaned.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b0abde9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def construct_property_graph(df, year_threshold=2017):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LINK ANALYSIS TASK I: PROPERTY GRAPH CONSTRUCTION\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Filter data for Jan 2017 onwards\n",
    "    df_filtered = df[df[\"year_only\"] >= year_threshold].copy()\n",
    "    print(\n",
    "        f\"Filtered data: {len(df_filtered):,} transactions from {year_threshold} onwards\"\n",
    "    )\n",
    "    \n",
    "    # Sample data for computational efficiency (use full dataset for production)\n",
    "    # Using stratified sampling to maintain distribution\n",
    "    sample_size = min(2000, len(df_filtered))\n",
    "    df_sample = df_filtered.sample(n=sample_size, random_state=42)\n",
    "    print(f\"Using sample of {sample_size:,} transactions for graph analysis\")\n",
    "    \n",
    "    # Reset index to use as node IDs\n",
    "    df_sample = df_sample.reset_index(drop=True)\n",
    "\n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "     # Add nodes with attributes\n",
    "    print(\"\\nAdding nodes to graph...\")\n",
    "    for idx, row in df_sample.iterrows():\n",
    "        G.add_node(\n",
    "            idx,\n",
    "            address=row[\"address\"],\n",
    "            town=row[\"town\"],\n",
    "            road=row[\"road\"],\n",
    "            flat_type=row[\"flat_type\"],\n",
    "            sqft=row[\"floor_area_sqft\"],\n",
    "            lease_remaining=row[\"remaining_lease\"],\n",
    "            price=row[\"resale_price\"],\n",
    "            psf=row[\"psf\"],\n",
    "            year_of_sale=row[\"year_only\"],\n",
    "            month_of_sale=row[\"month_only\"],\n",
    "            minimum_floor=row[\"storey_min\"],\n",
    "        )\n",
    "    print(f\"Added {G.number_of_nodes():,} nodes\")\n",
    "    \n",
    "    # Add edges based on similarity criteria (optimized approach)\n",
    "    print(\"\\nAdding edges based on similarity criteria...\")\n",
    "    edge_types = []\n",
    "    edges_added = 0\n",
    "    \n",
    "    # Group by town and flat_type for efficient edge creation\n",
    "    for (town, flat_type), group_df in df_sample.groupby([\"town\", \"flat_type\"]):\n",
    "        group_indices = group_df.index.tolist()\n",
    "\n",
    "        # Connect properties within the same town and flat type\n",
    "        for i_idx, i in enumerate(group_indices):\n",
    "            for j in group_indices[i_idx + 1 :]:\n",
    "                edge_weight = 3  # Already same town (1) + same flat_type (2)\n",
    "                edge_reasons = [\"same_town\", \"same_flat_type\"]\n",
    "\n",
    "                # Same road (weight: 3)\n",
    "                if df_sample.loc[i, \"road\"] == df_sample.loc[j, \"road\"]:\n",
    "                    edge_weight += 3\n",
    "                    edge_reasons.append(\"same_road\")\n",
    "\n",
    "                # Similar lease remaining (within 5 years, weight: 1)\n",
    "                lease_diff = abs(\n",
    "                    df_sample.loc[i, \"remaining_lease\"]\n",
    "                    - df_sample.loc[j, \"remaining_lease\"]\n",
    "                )\n",
    "                if lease_diff <= 5:\n",
    "                    edge_weight += 1\n",
    "                    edge_reasons.append(\"similar_lease\")\n",
    "                    \n",
    "\n",
    "                # Add edge (already meets minimum weight of 3)\n",
    "                G.add_edge(i, j, weight=edge_weight, reasons=edge_reasons)\n",
    "                edge_types.extend(edge_reasons)\n",
    "                edges_added += 1\n",
    "\n",
    "                if edges_added % 10000 == 0:\n",
    "                    print(f\"  Added {edges_added:,} edges...\")\n",
    "\n",
    "    print(f\"\\nAdded {G.number_of_edges():,} edges\")\n",
    "\n",
    "    # Analyze edge type distribution\n",
    "    edge_type_counts = Counter(edge_types)\n",
    "    print(\"\\nEdge Type Distribution:\")\n",
    "    for edge_type, count in edge_type_counts.most_common():\n",
    "        print(f\"  {edge_type}: {count:,}\")\n",
    "        \n",
    "    # Graph statistics\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"GRAPH STATISTICS\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"Number of nodes: {G.number_of_nodes():,}\")\n",
    "    print(f\"Number of edges: {G.number_of_edges():,}\")\n",
    "    print(f\"Graph density: {nx.density(G):.6f}\")\n",
    "    print(f\"Average degree: {sum(dict(G.degree()).values()) / G.number_of_nodes():.2f}\")\n",
    "\n",
    "    # Connected components analysis\n",
    "    connected_components = list(nx.connected_components(G))\n",
    "    print(f\"\\nNumber of connected components: {len(connected_components)}\")\n",
    "\n",
    "    component_sizes = [len(comp) for comp in connected_components]\n",
    "    print(\n",
    "        f\"Largest component size: {max(component_sizes):,} nodes ({max(component_sizes)/G.number_of_nodes()*100:.1f}%)\"\n",
    "    )\n",
    "    print(f\"Smallest component size: {min(component_sizes):,} nodes\")\n",
    "    print(f\"Average component size: {np.mean(component_sizes):.2f} nodes\")\n",
    "\n",
    "    # Analyze largest component\n",
    "    largest_component = max(connected_components, key=len)\n",
    "    G_largest = G.subgraph(largest_component).copy()\n",
    "\n",
    "    print(f\"\\nLargest Component Analysis:\")\n",
    "    print(f\"  Nodes: {G_largest.number_of_nodes():,}\")\n",
    "    print(f\"  Edges: {G_largest.number_of_edges():,}\")\n",
    "    print(f\"  Average clustering coefficient: {nx.average_clustering(G_largest):.4f}\")\n",
    "    print(\n",
    "        f\"  Diameter: {nx.diameter(G_largest) if nx.is_connected(G_largest) else 'N/A (disconnected)'}\"\n",
    "    )\n",
    "\n",
    "    # Visualize component size distribution\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    fig.suptitle(\"Property Graph Analysis\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # Component size distribution\n",
    "    ax1 = axes[0]\n",
    "    component_sizes_sorted = sorted(component_sizes, reverse=True)[:20]\n",
    "    ax1.bar(\n",
    "        range(len(component_sizes_sorted)),\n",
    "        component_sizes_sorted,\n",
    "        color=\"steelblue\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    ax1.set_xlabel(\"Component Rank\")\n",
    "    ax1.set_ylabel(\"Component Size (number of nodes)\")\n",
    "    ax1.set_title(\"Top 20 Connected Components by Size\", fontweight=\"bold\")\n",
    "    ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Degree distribution\n",
    "    ax2 = axes[1]\n",
    "    degrees = [deg for node, deg in G.degree()]\n",
    "    ax2.hist(degrees, bins=50, color=\"coral\", alpha=0.7, edgecolor=\"black\")\n",
    "    ax2.set_xlabel(\"Node Degree\")\n",
    "    ax2.set_ylabel(\"Frequency\")\n",
    "    ax2.set_title(\"Degree Distribution\", fontweight=\"bold\")\n",
    "    ax2.axvline(\n",
    "        np.mean(degrees),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=f\"Mean: {np.mean(degrees):.1f}\",\n",
    "    )\n",
    "    ax2.legend()\n",
    "    ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"graph_construction_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\nVisualization saved to: graph_construction_analysis.png\")\n",
    "    \n",
    "    return G, df_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "addb9524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LINK ANALYSIS TASK I: PROPERTY GRAPH CONSTRUCTION\n",
      "================================================================================\n",
      "\n",
      "Filtered data: 80,164 transactions from 2017 onwards\n",
      "Using sample of 2,000 transactions for graph analysis\n",
      "\n",
      "Adding nodes to graph...\n",
      "Added 2,000 nodes\n",
      "\n",
      "Adding edges based on similarity criteria...\n",
      "  Added 10,000 edges...\n",
      "  Added 20,000 edges...\n",
      "  Added 30,000 edges...\n",
      "\n",
      "Added 34,254 edges\n",
      "\n",
      "Edge Type Distribution:\n",
      "  same_town: 34,254\n",
      "  same_flat_type: 34,254\n",
      "  similar_lease: 18,262\n",
      "  same_road: 3,037\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "GRAPH STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Number of nodes: 2,000\n",
      "Number of edges: 34,254\n",
      "Graph density: 0.017136\n",
      "Average degree: 34.25\n",
      "\n",
      "Number of connected components: 114\n",
      "Largest component size: 96 nodes (4.8%)\n",
      "Smallest component size: 1 nodes\n",
      "Average component size: 17.54 nodes\n",
      "\n",
      "Largest Component Analysis:\n",
      "  Nodes: 96\n",
      "  Edges: 4,560\n",
      "  Average clustering coefficient: 1.0000\n",
      "  Diameter: 1\n",
      "\n",
      "Visualization saved to: graph_construction_analysis.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<networkx.classes.graph.Graph at 0x1244259a0>,\n",
       "           month           town  flat_type   block         street_name  \\\n",
       " 0    2017-12-01       TAMPINES     5 ROOM   B-123          SIMEI ST 1   \n",
       " 1    2019-12-01       SENGKANG     5 ROOM  B-440C       FERNVALE LINK   \n",
       " 2    2020-09-01       SENGKANG     5 ROOM   B-155      RIVERVALE CRES   \n",
       " 3    2017-05-01      PASIR RIS     5 ROOM   B-194     PASIR RIS ST 12   \n",
       " 4    2020-03-01     QUEENSTOWN     4 ROOM    B-89           DAWSON RD   \n",
       " ...         ...            ...        ...     ...                 ...   \n",
       " 1995 2017-05-01       SENGKANG     4 ROOM  B-434B         FERNVALE RD   \n",
       " 1996 2020-08-01      SERANGOON     3 ROOM   B-203      SERANGOON CTRL   \n",
       " 1997 2019-10-01      TOA PAYOH     4 ROOM  B-139B    LOR 1A TOA PAYOH   \n",
       " 1998 2017-01-01        HOUGANG  EXECUTIVE   B-940       HOUGANG ST 92   \n",
       " 1999 2019-04-01  CHOA CHU KANG     5 ROOM  B-691A  CHOA CHU KANG CRES   \n",
       " \n",
       "      storey_range  floor_area_sqm         flat_model  lease_commence_date  \\\n",
       " 0        04 TO 06             121           Improved                 1988   \n",
       " 1        07 TO 09             113           Improved                 2015   \n",
       " 2        04 TO 06             110           Improved                 2005   \n",
       " 3        04 TO 06             140            Model A                 1993   \n",
       " 4        07 TO 09              83  Premium Apartment                 2016   \n",
       " ...           ...             ...                ...                  ...   \n",
       " 1995     16 TO 18              94  Premium Apartment                 2013   \n",
       " 1996     01 TO 03              64         Simplified                 1986   \n",
       " 1997     37 TO 39              91               DBSS                 2012   \n",
       " 1998     01 TO 03             141          Apartment                 1998   \n",
       " 1999     04 TO 06             110           Improved                 2003   \n",
       " \n",
       "       remaining_lease  resale_price  month_only  year_only  floor_area_sqft  \\\n",
       " 0                  69        550000          12       2017             1302   \n",
       " 1                  94        550000          12       2019             1216   \n",
       " 2                  84        403000           9       2020             1184   \n",
       " 3                  75        520000           5       2017             1506   \n",
       " 4                  95        760000           3       2020              893   \n",
       " ...               ...           ...         ...        ...              ...   \n",
       " 1995               95        435000           5       2017             1011   \n",
       " 1996               64        295000           8       2020              688   \n",
       " 1997               91        842000          10       2019              979   \n",
       " 1998               80        575000           1       2017             1517   \n",
       " 1999               83        380000           4       2019             1184   \n",
       " \n",
       "       storey_min  storey_max  psf_temp     psf                      address  \\\n",
       " 0              4           6  42242.70  422.42           123 SIMEI STREET 1   \n",
       " 1              7           9  45230.26  452.30           440C FERNVALE LINK   \n",
       " 2              4           6  34037.16  340.37       155 RIVERVALE CRESCENT   \n",
       " 3              4           6  34528.55  345.28      194 PASIR RIS STREET 12   \n",
       " 4              7           9  85106.38  851.06               89 DAWSON ROAD   \n",
       " ...          ...         ...       ...     ...                          ...   \n",
       " 1995          16          18  43026.71  430.26           434B FERNVALE ROAD   \n",
       " 1996           1           3  42877.91  428.77        203 SERANGOON CENTRAL   \n",
       " 1997          37          39  86006.13  860.06     139B LORONG 1A TOA PAYOH   \n",
       " 1998           1           3  37903.76  379.03        940 HOUGANG STREET 92   \n",
       " 1999           4           6  32094.59  320.94  691A CHOA CHU KANG CRESCENT   \n",
       " \n",
       "                         road  \n",
       " 0             SIMEI STREET 1  \n",
       " 1              FERNVALE LINK  \n",
       " 2         RIVERVALE CRESCENT  \n",
       " 3        PASIR RIS STREET 12  \n",
       " 4                DAWSON ROAD  \n",
       " ...                      ...  \n",
       " 1995           FERNVALE ROAD  \n",
       " 1996       SERANGOON CENTRAL  \n",
       " 1997     LORONG 1A TOA PAYOH  \n",
       " 1998       HOUGANG STREET 92  \n",
       " 1999  CHOA CHU KANG CRESCENT  \n",
       " \n",
       " [2000 rows x 20 columns])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "construct_property_graph(registration_date_jan_2017_df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b9384135",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_graph_algorithms(G, df_sample):\n",
    "    \"\"\"\n",
    "    Run various graph algorithms on the property graph\n",
    "\n",
    "    Algorithms:\n",
    "    1. PageRank: Identify influential properties\n",
    "    2. Community Detection (Louvain): Find property clusters\n",
    "    3. Betweenness Centrality: Find bridge properties\n",
    "    4. Degree Centrality: Find well-connected properties\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"LINK ANALYSIS TASK II: GRAPH ALGORITHMS\")\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "    # Algorithm 1: PageRank\n",
    "    print(\"-\" * 80)\n",
    "    print(\"ALGORITHM 1: PageRank\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nHow it works:\")\n",
    "    print(\"PageRank is an algorithm originally developed by Google to rank web pages.\")\n",
    "    print(\"It works by counting the number and quality of connections to a node to\")\n",
    "    print(\"determine a rough estimate of how 'important' that node is. The algorithm\")\n",
    "    print(\"assumes that more important nodes are likely to receive more connections.\")\n",
    "    print(\"\\nIn our property graph context:\")\n",
    "    print(\"- Nodes with high PageRank are properties that are highly connected to\")\n",
    "    print(\"  other well-connected properties\")\n",
    "    print(\"- These represent 'typical' or 'benchmark' properties in the market\")\n",
    "    print(\"- They share many characteristics with other properties\")\n",
    "\n",
    "    pagerank = nx.pagerank(G, weight=\"weight\")\n",
    "    df_sample[\"pagerank\"] = df_sample.index.map(pagerank)\n",
    "\n",
    "    print(\"\\nTop 10 Properties by PageRank:\")\n",
    "    top_pagerank = df_sample.nlargest(10, \"pagerank\")[\n",
    "        [\"address\", \"town\", \"flat_type\", \"resale_price\", \"psf\", \"pagerank\"]\n",
    "    ]\n",
    "    print(top_pagerank.to_string(index=False))\n",
    "\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\"Properties with high PageRank scores are 'typical' market representatives.\")\n",
    "    print(\"They share multiple characteristics with many other properties, making them\")\n",
    "    print(\"good reference points for price comparisons and market analysis.\")\n",
    "\n",
    "    # Algorithm 2: Community Detection\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"ALGORITHM 2: Community Detection (Greedy Modularity)\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nHow it works:\")\n",
    "    print(\"Community detection algorithms identify groups of nodes that are more\")\n",
    "    print(\"densely connected to each other than to nodes in other groups. The\")\n",
    "    print(\"greedy modularity algorithm works by iteratively merging communities\")\n",
    "    print(\"in a way that maximizes modularity - a measure of how well the network\")\n",
    "    print(\"is divided into communities.\")\n",
    "    print(\"\\nIn our property graph context:\")\n",
    "    print(\"- Communities represent clusters of similar properties\")\n",
    "    print(\"- Properties in the same community share multiple characteristics\")\n",
    "    print(\"- This helps identify distinct market segments\")\n",
    "\n",
    "    # Use largest connected component for community detection\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    G_largest = G.subgraph(largest_cc).copy()\n",
    "\n",
    "    communities = nx.community.greedy_modularity_communities(G_largest, weight=\"weight\")\n",
    "\n",
    "    # Map communities back to dataframe\n",
    "    community_map = {}\n",
    "    for comm_id, community in enumerate(communities):\n",
    "        for node in community:\n",
    "            community_map[node] = comm_id\n",
    "\n",
    "    df_sample[\"community\"] = df_sample.index.map(community_map)\n",
    "\n",
    "    print(f\"\\nNumber of communities detected: {len(communities)}\")\n",
    "    print(f\"\\nCommunity size distribution:\")\n",
    "    for i, community in enumerate(sorted(communities, key=len, reverse=True)[:10]):\n",
    "        print(f\"  Community {i}: {len(community)} properties\")\n",
    "\n",
    "    # Analyze community characteristics\n",
    "    print(\"\\nCommunity Characteristics (Top 5 by size):\")\n",
    "    for i, community in enumerate(sorted(communities, key=len, reverse=True)[:5]):\n",
    "        comm_nodes = list(community)\n",
    "        comm_data = df_sample[df_sample.index.isin(comm_nodes)]\n",
    "        print(f\"\\nCommunity {i} ({len(community)} properties):\")\n",
    "        print(\n",
    "            f\"  Most common town: {comm_data['town'].mode().values[0] if len(comm_data) > 0 else 'N/A'}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Most common flat type: {comm_data['flat_type'].mode().values[0] if len(comm_data) > 0 else 'N/A'}\"\n",
    "        )\n",
    "        print(f\"  Average price: ${comm_data['resale_price'].mean():,.0f}\")\n",
    "        print(f\"  Average PSF: ${comm_data['psf'].mean():.2f}\")\n",
    "        print(\n",
    "            f\"  Average lease remaining: {comm_data['lease_remaining'].mean():.1f} years\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\n",
    "        \"Communities reveal natural market segments. Properties in the same community\"\n",
    "    )\n",
    "    print(\n",
    "        \"share similar characteristics and pricing patterns, helping identify comparable\"\n",
    "    )\n",
    "    print(\"properties for valuation and understanding market sub-segments.\")\n",
    "\n",
    "    # Algorithm 3: Betweenness Centrality\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"ALGORITHM 3: Betweenness Centrality\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nHow it works:\")\n",
    "    print(\"Betweenness centrality measures how often a node appears on the shortest\")\n",
    "    print(\"paths between other nodes in the network. It's calculated by finding all\")\n",
    "    print(\"shortest paths between all pairs of nodes, then counting how many times\")\n",
    "    print(\"each node appears on these paths.\")\n",
    "    print(\"\\nIn our property graph context:\")\n",
    "    print(\"- High betweenness means a property 'bridges' different market segments\")\n",
    "    print(\"- These properties share characteristics with diverse property types\")\n",
    "    print(\"- They can serve as transition points between different market segments\")\n",
    "\n",
    "    # Calculate for a sample (computationally expensive for large graphs)\n",
    "    sample_nodes = list(G_largest.nodes())[:1000]  # Sample for efficiency\n",
    "    G_sample = G_largest.subgraph(sample_nodes).copy()\n",
    "    betweenness = nx.betweenness_centrality(G_sample, weight=\"weight\")\n",
    "\n",
    "    df_sample[\"betweenness\"] = df_sample.index.map(betweenness)\n",
    "\n",
    "    print(\"\\nTop 10 Properties by Betweenness Centrality:\")\n",
    "    top_betweenness = df_sample[df_sample[\"betweenness\"].notna()].nlargest(\n",
    "        10, \"betweenness\"\n",
    "    )[[\"address\", \"town\", \"flat_type\", \"resale_price\", \"psf\", \"betweenness\"]]\n",
    "    print(top_betweenness.to_string(index=False))\n",
    "\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\n",
    "        \"Properties with high betweenness centrality bridge different market segments.\"\n",
    "    )\n",
    "    print(\"They have characteristics that connect otherwise disparate property groups,\")\n",
    "    print(\"making them valuable reference points for cross-segment price comparisons.\")\n",
    "\n",
    "    # Algorithm 4: Degree Centrality\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"ALGORITHM 4: Degree Centrality\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"\\nHow it works:\")\n",
    "    print(\"Degree centrality simply counts the number of connections each node has.\")\n",
    "    print(\"It's normalized by dividing by the maximum possible degree (n-1, where n\")\n",
    "    print(\"is the number of nodes), giving a value between 0 and 1.\")\n",
    "    print(\"\\nIn our property graph context:\")\n",
    "    print(\"- High degree means a property is similar to many other properties\")\n",
    "    print(\"- These are 'mainstream' properties with common characteristics\")\n",
    "    print(\"- Low degree properties are more unique or specialized\")\n",
    "\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    df_sample[\"degree_centrality\"] = df_sample.index.map(degree_centrality)\n",
    "\n",
    "    print(\"\\nTop 10 Properties by Degree Centrality:\")\n",
    "    top_degree = df_sample.nlargest(10, \"degree_centrality\")[\n",
    "        [\"address\", \"town\", \"flat_type\", \"resale_price\", \"psf\", \"degree_centrality\"]\n",
    "    ]\n",
    "    print(top_degree.to_string(index=False))\n",
    "\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\n",
    "        \"Properties with high degree centrality are the most 'typical' in the market.\"\n",
    "    )\n",
    "    print(\"They have many similar properties, making them easier to value based on\")\n",
    "    print(\n",
    "        \"comparables. Low-degree properties are more unique and may require specialized\"\n",
    "    )\n",
    "    print(\"valuation approaches.\")\n",
    "\n",
    "    # Visualize algorithm results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle(\"Graph Algorithm Results\", fontsize=16, fontweight=\"bold\")\n",
    "\n",
    "    # PageRank distribution\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.hist(\n",
    "        df_sample[\"pagerank\"].dropna(),\n",
    "        bins=50,\n",
    "        color=\"steelblue\",\n",
    "        alpha=0.7,\n",
    "        edgecolor=\"black\",\n",
    "    )\n",
    "    ax1.set_xlabel(\"PageRank Score\")\n",
    "    ax1.set_ylabel(\"Frequency\")\n",
    "    ax1.set_title(\"PageRank Distribution\", fontweight=\"bold\")\n",
    "    ax1.axvline(\n",
    "        df_sample[\"pagerank\"].mean(),\n",
    "        color=\"red\",\n",
    "        linestyle=\"--\",\n",
    "        linewidth=2,\n",
    "        label=f'Mean: {df_sample[\"pagerank\"].mean():.6f}',\n",
    "    )\n",
    "    ax1.legend()\n",
    "    ax1.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Community sizes\n",
    "    ax2 = axes[0, 1]\n",
    "    community_sizes = (\n",
    "        df_sample[\"community\"].value_counts().sort_values(ascending=False).head(15)\n",
    "    )\n",
    "    ax2.bar(\n",
    "        range(len(community_sizes)), community_sizes.values, color=\"coral\", alpha=0.7\n",
    "    )\n",
    "    ax2.set_xlabel(\"Community ID (Ranked)\")\n",
    "    ax2.set_ylabel(\"Number of Properties\")\n",
    "    ax2.set_title(\"Top 15 Community Sizes\", fontweight=\"bold\")\n",
    "    ax2.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "    # Betweenness centrality vs Price\n",
    "    ax3 = axes[1, 0]\n",
    "    df_plot = df_sample[df_sample[\"betweenness\"].notna()]\n",
    "    scatter = ax3.scatter(\n",
    "        df_plot[\"betweenness\"],\n",
    "        df_plot[\"resale_price\"],\n",
    "        c=df_plot[\"psf\"],\n",
    "        cmap=\"viridis\",\n",
    "        alpha=0.6,\n",
    "        s=30,\n",
    "    )\n",
    "    ax3.set_xlabel(\"Betweenness Centrality\")\n",
    "    ax3.set_ylabel(\"Price ($)\")\n",
    "    ax3.set_title(\"Betweenness Centrality vs Price\", fontweight=\"bold\")\n",
    "    ax3.grid(alpha=0.3)\n",
    "    cbar = plt.colorbar(scatter, ax=ax3)\n",
    "    cbar.set_label(\"PSF ($)\")\n",
    "\n",
    "    # Degree centrality vs Price\n",
    "    ax4 = axes[1, 1]\n",
    "    scatter2 = ax4.scatter(\n",
    "        df_sample[\"degree_centrality\"],\n",
    "        df_sample[\"resale_price\"],\n",
    "        c=df_sample[\"psf\"],\n",
    "        cmap=\"plasma\",\n",
    "        alpha=0.6,\n",
    "        s=30,\n",
    "    )\n",
    "    ax4.set_xlabel(\"Degree Centrality\")\n",
    "    ax4.set_ylabel(\"Price ($)\")\n",
    "    ax4.set_title(\"Degree Centrality vs Price\", fontweight=\"bold\")\n",
    "    ax4.grid(alpha=0.3)\n",
    "    cbar2 = plt.colorbar(scatter2, ax=ax4)\n",
    "    cbar2.set_label(\"PSF ($)\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"graph_algorithms_analysis.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "\n",
    "    print(f\"\\n\\nVisualization saved to: graph_algorithms_analysis.png\")\n",
    "\n",
    "    return df_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "52822825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LINK ANALYSIS TASK I: PROPERTY GRAPH CONSTRUCTION\n",
      "================================================================================\n",
      "\n",
      "Filtered data: 80,164 transactions from 2017 onwards\n",
      "Using sample of 2,000 transactions for graph analysis\n",
      "\n",
      "Adding nodes to graph...\n",
      "Added 2,000 nodes\n",
      "\n",
      "Adding edges based on similarity criteria...\n",
      "  Added 10,000 edges...\n",
      "  Added 20,000 edges...\n",
      "  Added 30,000 edges...\n",
      "\n",
      "Added 34,254 edges\n",
      "\n",
      "Edge Type Distribution:\n",
      "  same_town: 34,254\n",
      "  same_flat_type: 34,254\n",
      "  similar_lease: 18,262\n",
      "  same_road: 3,037\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "GRAPH STATISTICS\n",
      "--------------------------------------------------------------------------------\n",
      "Number of nodes: 2,000\n",
      "Number of edges: 34,254\n",
      "Graph density: 0.017136\n",
      "Average degree: 34.25\n",
      "\n",
      "Number of connected components: 114\n",
      "Largest component size: 96 nodes (4.8%)\n",
      "Smallest component size: 1 nodes\n",
      "Average component size: 17.54 nodes\n",
      "\n",
      "Largest Component Analysis:\n",
      "  Nodes: 96\n",
      "  Edges: 4,560\n",
      "  Average clustering coefficient: 1.0000\n",
      "  Diameter: 1\n",
      "\n",
      "Visualization saved to: graph_construction_analysis.png\n",
      "\n",
      "================================================================================\n",
      "LINK ANALYSIS TASK II: GRAPH ALGORITHMS\n",
      "================================================================================\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ALGORITHM 1: PageRank\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "How it works:\n",
      "PageRank is an algorithm originally developed by Google to rank web pages.\n",
      "It works by counting the number and quality of connections to a node to\n",
      "determine a rough estimate of how 'important' that node is. The algorithm\n",
      "assumes that more important nodes are likely to receive more connections.\n",
      "\n",
      "In our property graph context:\n",
      "- Nodes with high PageRank are properties that are highly connected to\n",
      "  other well-connected properties\n",
      "- These represent 'typical' or 'benchmark' properties in the market\n",
      "- They share many characteristics with other properties\n",
      "\n",
      "Top 10 Properties by PageRank:\n",
      "                   address        town flat_type  resale_price    psf  pagerank\n",
      "       204 MARSILING DRIVE   WOODLANDS    3 ROOM        235000 321.47  0.000593\n",
      "     31 TEBAN GARDENS ROAD JURONG EAST    4 ROOM        355000 333.33  0.000591\n",
      "          28 GHIM MOH LINK  QUEENSTOWN    4 ROOM        820000 828.28  0.000585\n",
      "          23 GHIM MOH LINK  QUEENSTOWN    4 ROOM        665000 671.71  0.000585\n",
      "          28 GHIM MOH LINK  QUEENSTOWN    4 ROOM        630000 636.36  0.000585\n",
      "        32 MARSILING DRIVE   WOODLANDS    3 ROOM        285000 363.05  0.000585\n",
      "723 CLEMENTI WEST STREET 2    CLEMENTI    5 ROOM        560000 440.94  0.000582\n",
      "     409 PASIR RIS DRIVE 6   PASIR RIS    4 ROOM        455000 402.65  0.000581\n",
      "  712 BEDOK RESERVOIR ROAD       BEDOK    4 ROOM        350000 349.65  0.000579\n",
      "        6A BOON TIONG ROAD BUKIT MERAH    5 ROOM        950000 767.98  0.000579\n",
      "\n",
      "Interpretation:\n",
      "Properties with high PageRank scores are 'typical' market representatives.\n",
      "They share multiple characteristics with many other properties, making them\n",
      "good reference points for price comparisons and market analysis.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "ALGORITHM 2: Community Detection (Greedy Modularity)\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "How it works:\n",
      "Community detection algorithms identify groups of nodes that are more\n",
      "densely connected to each other than to nodes in other groups. The\n",
      "greedy modularity algorithm works by iteratively merging communities\n",
      "in a way that maximizes modularity - a measure of how well the network\n",
      "is divided into communities.\n",
      "\n",
      "In our property graph context:\n",
      "- Communities represent clusters of similar properties\n",
      "- Properties in the same community share multiple characteristics\n",
      "- This helps identify distinct market segments\n",
      "\n",
      "Number of communities detected: 3\n",
      "\n",
      "Community size distribution:\n",
      "  Community 0: 40 properties\n",
      "  Community 1: 36 properties\n",
      "  Community 2: 20 properties\n",
      "\n",
      "Community Characteristics (Top 5 by size):\n",
      "\n",
      "Community 0 (40 properties):\n",
      "  Most common town: SENGKANG\n",
      "  Most common flat type: 4 ROOM\n",
      "  Average price: $405,179\n",
      "  Average PSF: $411.02\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'lease_remaining'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/htx_xdata_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'lease_remaining'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run graph analysis\u001b[39;00m\n\u001b[32m      2\u001b[39m G, df_graph = construct_property_graph(registration_date_jan_2017_df_cleaned, year_threshold=\u001b[32m2017\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df_graph_results = \u001b[43mrun_graph_algorithms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_graph\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mrun_graph_algorithms\u001b[39m\u001b[34m(G, df_sample)\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Average price: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomm_data[\u001b[33m'\u001b[39m\u001b[33mresale_price\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,.0f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     91\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Average PSF: $\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomm_data[\u001b[33m'\u001b[39m\u001b[33mpsf\u001b[39m\u001b[33m'\u001b[39m].mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     92\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Average lease remaining: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mcomm_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlease_remaining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.mean()\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m years\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     94\u001b[39m     )\n\u001b[32m     96\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mInterpretation:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     97\u001b[39m \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCommunities reveal natural market segments. Properties in the same community\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/htx_xdata_env/lib/python3.12/site-packages/pandas/core/frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/htx_xdata_env/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'lease_remaining'"
     ]
    }
   ],
   "source": [
    "# Run graph analysis\n",
    "G, df_graph = construct_property_graph(registration_date_jan_2017_df_cleaned, year_threshold=2017)\n",
    "df_graph_results = run_graph_algorithms(G, df_graph)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htx_xdata_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
